---
title: agent学习(1) 
date: 2024-09-16 17:56:35
tags: 人工智能
cover: https://resource-un4.pages.dev/article/output_max_compressed.webp
categories: Learn
copyright: true
---

# Agent学习

人工智能是最新兴的科学与工程领域之一，它是使机器能够像人一样思考、像人一样行动，并且这些思考和行动都是合理的；它的研究目标是试图理解智能实体，并且试图建造智能实体。人工智能既是科学也是工程，它与任何智力工作相关，所以它是一个普遍的研究领域，是一门科学，同时它还涉及对智能系统的开发和应用，可以说它是一门工程。

简单分析两种类型的智能体（agent）：基于效用的agent和基于目标的agent

## 基于目标的agent

	目标导向型智能体会根据其所处环境来选择一系列动作，以实现某个具体目标。它不考虑动作的长期效用，只要能够达到目标即可。

**伪代码步骤**：

1. 通过感知(Sensors)和预设程序获取当前状态(State) 、世界运行规则、可以做出的动作。
2. 判断状态是否达成目标
3. 如果达成目标，结束。
4. 如果未达成目标，判断各个动作后状态的改变，从中选择可以接近目标的动作。
5. 执行动作，更新状态。

```python
class GoalBasedAgent:
    def __init__(self, initial_state, goal_state):
        self.state = initial_state
        self.goal_state = goal_state
def perceive_environment(self):
    # 获取当前环境状态
    return self.state

def goal_achieved(self):
    # 判断是否达成目标
    return self.state == self.goal_state

def choose_action(self, state):
    # 根据当前状态选择下一步动作
    # 动作应让 agent 更接近目标
    # 这里的 action 逻辑根据具体问题不同而不同
    possible_actions = ['action1', 'action2', 'action3']
    for action in possible_actions:
        if action is helpful:
            break
    return action


def update_state(self, action):
    # 根据动作更新状态
    self.state = self.state_transition(action)

def state_transition(self, action):
    # 模拟状态转移（不同问题的实现不同）
    return action

def run(self):
    while not self.goal_achieved():
        current_state = self.perceive_environment()
        action = self.choose_action(current_state)
        self.update_state(action)
    print("Goal achieved!")
```

## 基于效用的agent

​	基于效用的智能体不仅仅追求达到某个目标，还会通过一个效用函数来评估动作的收益，选择能最大化效用的动作。它可以权衡多种可能的动作和路径，选择最优的行动策略。这与基于目标的agent实现方式非常相似，不同的地方在于行动的选择上。

​	基于目标的agent智能做出动作选择时，能够选择的动作只有“好”与“不好”。当有多个“好”的行动时，并不能实现选择“最好”的行动。

​	基于效用的agent会通过效用(Utility)指标，对每个动作进行评估后，选择出“最好”的行动去执行。

​	伪代码与基于上文仅仅选择行动上有差异.

    class UtilityBasedAgent:
        def __init__(self, initial_state, goal_state, utility_function):
            self.state = initial_state
            self.goal_state = goal_state
            self.utility_function = utility_function
    	def perceive_environment(self):
        # 获取当前环境状态
        	return self.state
    
        def goal_achieved(self):
            # 判断是否达成目标
            return self.state == self.goal_state
    
        def evaluate_actions(self, state):
            # 根据效用函数评估每个可能动作的效用值
            possible_actions = ['action1', 'action2', 'action3']
            best_action = None
            max_utility = float('-inf')
            for action in possible_actions:
                utility = self.utility_function(state, action)
                if utility > max_utility:
                    max_utility = utility
                    best_action = action
            return best_action
    
        def update_state(self, action):
            # 根据动作更新状态
            self.state = self.state_transition(action)
    
        def state_transition(self, action):
            # 模拟状态转移
            return action
    
        def run(self):
            while not self.goal_achieved():
                current_state = self.perceive_environment()
                action = self.evaluate_actions(current_state)
                self.update_state(action)
            print("Goal achieved with maximum utility!")

## 一个基于吃豆人agent的简单示例

定义一个吃逗人pacman的类，吃豆人包括

1.自身的状态 和 动作空间 ，目标状态 最高分 （分就是效用）.

```
import heapq

# 定义地图，' '代表空地，'#'代表墙壁，'.'代表豆子，'P'代表吃豆人起始位置
map_grid = [
    ['#', '#', '#', '#', '#', '#', '#'],
    ['#', '.', '.', '.', '.', '.', '#'],
    ['#', '.', '#', '#', '#', '.', '#'],
    ['#', '.', '.', ' ', '.', '.', '#'],
    ['#', '.', '.', '.', '.', 'P', '#'],
    ['#', '.', '#', '#', '#', '.', '#'],
    ['#', '#', '#', '#', '#', '#', '#']
]

# 定义方向：上，下，左，右
DIRECTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]

# A* 搜索算法
def a_star_search(grid, start, end):
    rows, cols = len(grid), len(grid[0])
    open_set = []
    heapq.heappush(open_set, (0, start))
    g_score = {start: 0}
    f_score = {start: heuristic(start, end)}
    came_from = {}

    while open_set:
        current = heapq.heappop(open_set)[1]

        if current == end:
            return reconstruct_path(came_from, current)

        for dx, dy in DIRECTIONS:
            neighbor = (current[0] + dx, current[1] + dy)
            if 0 <= neighbor[0] < rows and 0 <= neighbor[1] < cols:
                if grid[neighbor[0]][neighbor[1]] == '#':
                    continue  # 墙壁，跳过

                tentative_g_score = g_score[current] + 1

                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:
                    came_from[neighbor] = current
                    g_score[neighbor] = tentative_g_score
                    f_score[neighbor] = tentative_g_score + heuristic(neighbor, end)
                    heapq.heappush(open_set, (f_score[neighbor], neighbor))

    return []

def heuristic(a, b):
    return abs(a[0] - b[0]) + abs(a[1] - b[1])

def reconstruct_path(came_from, current):
    path = [current]
    while current in came_from:
        current = came_from[current]
        path.append(current)
    return path[::-1]

# 吃豆人 Agent
class PacManAgent:
    def __init__(self, grid):
        self.grid = grid
        self.position = self.find_pacman(grid)

    def find_pacman(self, grid):
        for i in range(len(grid)):
            for j in range(len(grid[0])):
                if grid[i][j] == 'P':
                    return (i, j)

    def get_beans(self):
        beans = []
        for i in range(len(self.grid)):
            for j in range(len(self.grid[0])):
                if self.grid[i][j] == '.':
                    beans.append((i, j))
        return beans

    def move(self):
        beans = self.get_beans()
        if not beans:
            print("All beans eaten!")
            return

        nearest_bean = min(beans, key=lambda bean: heuristic(self.position, bean))
        path = a_star_search(self.grid, self.position, nearest_bean)

        if path:
            next_move = path[1]
            self.grid[self.position[0]][self.position[1]] = ' '
            self.position = next_move
            if self.grid[next_move[0]][next_move[1]] == '.':
                self.grid[next_move[0]][next_move[1]] = 'P'
            else:
                self.grid[next_move[0]][next_move[1]] = 'P'
                
        self.print_grid()

    def print_grid(self):
        for row in self.grid:
            print(''.join(row))
        print()

# 初始化吃豆人 Agent 并运行
pacman =  PacManAgent(map_grid)
while True:
    pacman.move()
    # 为了避免无限循环，当没有豆子时停止
    if not pacman.get_beans():
        break
```

这个代码实现了一个使用A*搜索算法的吃豆人(Pac-Man) Agent，负责在二维网格地图中寻找并吃掉所有豆子。代码分为多个部分，涉及地图初始化、A*搜索算法、吃豆人Agent的动作和移动逻辑。

### 代码分解

1. **地图定义 (`map_grid`)**
    ```python
    map_grid = [
        ['#', '#', '#', '#', '#', '#', '#'],
        ['#', '.', '.', '.', '.', '.', '#'],
        ['#', '.', '#', '#', '#', '.', '#'],
        ['#', '.', '.', ' ', '.', '.', '#'],
        ['#', '.', '.', '.', '.', 'P', '#'],
        ['#', '.', '#', '#', '#', '.', '#'],
        ['#', '#', '#', '#', '#', '#', '#']
    ]
    ```
    - 这个二维数组定义了吃豆人的游戏地图。`'#'`表示墙壁，`'.'`表示豆子，`'P'`是吃豆人的起始位置，`' '`是空地。
    - 吃豆人从起始位置(`'P'`)出发，目标是吃掉所有豆子(`'.'`)。

2. **方向定义 (`DIRECTIONS`)**
    ```python
    DIRECTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]
    ```
    - 定义了吃豆人的四个移动方向：上、下、左、右，分别用坐标偏移来表示。
        - `(-1, 0)` 表示上移一格
        - `(1, 0)` 表示下移一格
        - `(0, -1)` 表示左移一格
        - `(0, 1)` 表示右移一格

3. **A* 搜索算法 (`a_star_search`)**
    ```python
    def a_star_search(grid, start, end):
        ...
    ```
    - 这是A*路径搜索算法，用于找到从起点到目标点（豆子）之间的最短路径。
    - **主要步骤**：
        - 使用 `heapq` 实现的优先队列维护待搜索的节点（`open_set`），每次从队列中取出估计距离最短的节点进行扩展。
        - `g_score` 记录当前节点到起点的代价，`f_score` 估算从当前节点经过目标点的总代价（启发式函数+实际代价）。
        - 对每个节点的邻居（上下左右）进行搜索，更新代价，如果找到目标则返回路径。
    
    - **关键函数**：
        - `heuristic(a, b)`：使用曼哈顿距离作为启发式函数，计算当前节点到目标节点的预估距离。
        - `reconstruct_path(came_from, current)`：当找到目标时，重建从起点到终点的路径。

4. **吃豆人 Agent (`PacManAgent`)**
    ```python
    class PacManAgent:
        def __init__(self, grid):
            self.grid = grid
            self.position = self.find_pacman(grid)
    ```
    - 吃豆人 Agent 负责在地图上移动，吃掉所有豆子。
    - **主要方法**：
        - `find_pacman(grid)`：在地图中查找吃豆人的初始位置（'P'）。
        - `get_beans()`：扫描整个地图，找到所有豆子的位置。
        - `move()`：找到离吃豆人最近的豆子，并使用A*算法计算最短路径，然后移动吃豆人到豆子的位置。
        - `print_grid()`：打印当前地图，用于显示吃豆人的当前位置和地图的状态变化。

5. **查找吃豆人的位置 (`find_pacman`)**
    ```python
    def find_pacman(self, grid):
        for i in range(len(grid)):
            for j in range(len(grid[0])):
                if grid[i][j] == 'P':
                    return (i, j)
    ```
    - 遍历整个地图，找到吃豆人的初始位置 `'P'`，并返回其坐标。

6. **获取所有豆子的位置 (`get_beans`)**
   
    ```python
    def get_beans(self):
        beans = []
        for i in range(len(self.grid)):
            for j in range(len(self.grid[0])):
                if self.grid[i][j] == '.':
                    beans.append((i, j))
        return beans
    ```
    - 遍历地图，找到所有豆子（`'.'`）的位置，并返回包含所有豆子坐标的列表。
    
7. **吃豆人移动 (`move`)**
   
    ```python
    def move(self):
        beans = self.get_beans()
        if not beans:
            print("All beans eaten!")
            return
    
        nearest_bean = min(beans, key=lambda bean: heuristic(self.position, bean))
        path = a_star_search(self.grid, self.position, nearest_bean)
    
        if path:
            next_move = path[1]
            self.grid[self.position[0]][self.position[1]] = ' '
            self.position = next_move
            if self.grid[next_move[0]][next_move[1]] == '.':
                self.grid[next_move[0]][next_move[1]] = 'P'
            else:
                self.grid[next_move[0]][next_move[1]] = 'P'
    
        self.print_grid()
    ```
    - **逻辑流程**：
        1. 调用 `get_beans()` 获取当前地图上的所有豆子。
        2. 如果没有豆子，打印 "All beans eaten!" 并停止。
        3. 使用 `min()` 和 `heuristic()` 找到最近的豆子。
        4. 调用 `a_star_search()` 计算吃豆人到该豆子的最短路径。
        5. 移动吃豆人到路径上的下一个位置，更新地图：
            - 吃豆人当前位置变为空地 `' '`。
            - 吃豆人移动到的新位置变为 `'P'`（如果该位置有豆子，则吃掉豆子）。
        6. 打印当前地图状态。
    
8. **运行吃豆人 Agent**
   
    ```python
    pacman = PacManAgent(map_grid)
    while True:
        pacman.move()
        if not pacman.get_beans():
            break
    ```
    - 创建吃豆人 Agent，传入初始地图。
    - 不断调用 `pacman.move()` 让吃豆人寻找豆子并移动，直到地图上没有豆子为止。

---

### 运行逻辑：

1. 创建吃豆人 Agent，传入初始地图。
2. 不断调用 `pacman.move()`
3.  (`get_beans`)遍历地图，找到所有豆子（`'.'`）的位置，并返回包含所有豆子坐标的列表。
4. 使用 `min()` 和 `heuristic()` 找到最近的豆子。
5. 调用 `a_star_search()` 计算吃豆人到该豆子的最短路径。
	1. 使用 `heapq` 实现的优先队列维护待搜索的节点（`open_set`），每次从队列中取出估计距离最短的节点进行扩展。
	2. `g_score` 记录当前节点到起点的代价，`f_score` 估算从当前节点经过目标点的总代价（启发式函数+实际代价）。
	3. 对每个节点的邻居（上下左右）进行搜索，更新代价，如果找到目标则返回路径。

6. 移动吃豆人到路径上的下一个位置，更新地图：
	- 吃豆人当前位置变为空地 `' '`。
	- 吃豆人移动到的新位置变为 `'P'`（如果该位置有豆子，则吃掉豆子）。

7. 打印当前地图状态。

### 总结：

- 该代码使用了A*搜索算法帮助吃豆人Agent在地图上移动，寻找并吃掉所有豆子。
- `move()` 函数根据吃豆人当前位置，通过启发式算法找到最近的豆子，并沿最短路径前进。
- `a_star_search()` 负责寻找最优路径，`get_beans()` 则找到地图上的所有豆子。
- 最后，代码会不断执行吃豆人的移动，直到地图上所有豆子都被吃掉。

### 后续：

**a.** 添加鬼怪在地图上移动的逻辑，让吃豆人不仅要吃豆子，还要躲避鬼怪。  
**b.** 实现吃豆人吃掉大豆子后可以反过来追逐鬼怪的功能，模拟真实的吃豆人游戏体验。
